# BERT-TFBS: a novel BERT-based model for predicting transcription factor binding sites by transfer learning

## Table of Contents

1. [Introduction](#introduction)
2. [Python Environment](#python-environment)
3. [Project Structure](#Project-Structure)
   1. [Dataset](#Dataset)
   2. [Model](#Model)
   3. [script](#script)
---

## 1. Introduction

We present a novel deep learning model called BERT-TFBS which is designed for predicting transcription factor binding sites (TFBSs) solely based on DNA sequences. The model comprises a pre-trained BERT module (DNABERT-2), a convolutional neural network (CNN) module, a convolutional block attention module (CBAM), and an output module. The model is trained and tested on 165 ENCODE ChIP-seq datasets, demonstrating the superior ability of BERT-TFBS to predict TFBSs. Experimental results indicate that BERT-TFBS achieves an average accuracy of 0.851, a ROC-AUC of 0.919, and a PR-AUC of 0.920 for TFBS prediction. Here, the codes for implementing, training, and testing BERT-TFBS are provided.


## 2. Python Environment

Python 3.9 and packages version:

- torch==1.12.0
- torchvision==0.13.0
- transformers==4.22.2
- numpy==1.22.4
- pandas==1.4.4
- scikit-learn==1.1.1

## 3. Project Structure

### 3.1 **Dataset**

   For this study, we choose 165 ChIP-seq datasets generated by the Encyclopedia of DNA Elements (ENCODE) project as benchmark datasets, which encompass 29 different TFs from various cell lines. According to the work of Zeng et al, each of the datasets can be randomly divided into a training set (80\%) and the corresponding test set (20\%), where each positive sample is a 101 bp DNA sequence that has been experimentally confirmed to contain TFBSs, and each negative sample is the sequence that is obtained from a positive sequence through random permutations while preserving the nucleotide frequencies. This folder contains only the Hepg2 FOSL2 ChIP-seq dataset from the 165 chip-seq datasets. The original files are named `train.data` and `test.data`. We processe the raw data into `train.csv` and `test.csv` using the `pre_data.py` script, making it convenient for subsequent training.

### 3.2 **Model**
   -  The overall architectures of BERT-TFBS is presented in the following figure, which consists of a DNABERT-2 module, a CNN module, a CBAM, and an output module
     
      ![Model Architecture](https://github.com/ZX1998-12/BERT-TFBS/raw/master/Model/model.jpg)

   - `model.pth` is the BERT-TFBS model that is trained on the training subest of the Hepg2 FOSL2 ChIP-seq dataset.
     
   - The pre-trained BERT model is available at Huggingface as `zhihan1996/DNABERT-2-117M`.
     
     To load the model from Huggingface, we can use the following code:
     
     ```python
     import torch
     from transformers import AutoTokenizer, AutoModel
     
     tokenizer = AutoTokenizer.from_pretrained("zhihan1996/DNABERT-2-117M", trust_remote_code=True)
     model = AutoModel.from_pretrained("zhihan1996/DNABERT-2-117M", trust_remote_code=True)
     ```
   - To train the model, we can run `train.py` script using the training dataset.
     
     ```shell
     # Example code to train the model
     python train.py
     ```
     
     We can also run `test.py` to test the model

### 3.3 **script**
   - `dataloader.py` converts DNA sequences into token embeddings.
   - `CBAM.py` implements the CBAM which integrates channel attention and spatial attention mechanisms, enhancing the representation of local features by emphasizing important channels and spatial information.
   - `adjust_learning.py` is the learning rate of the optimizer which is adjusted by using warm-up and cosine annealing techniques.
   - `model.py` implements the BERT-TFBS which consists of a DNABERT-2 module, a CNN module, a CBAM, and an output module.
   - `model_V1.py` implements the BERT-TFBS-v1 variant model which is constructed by removing the CNN module, CBAM, and the convolutional layer in the output module from BERT-TFBS.
   - `model_V2.py` implements the BERT-TFBS-v2 variant model which is constructed by removing CBAM from BERT-TFBS.
